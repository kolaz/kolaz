#!/bin/bash

# shellcheck disable=SC2001
# shellcheck disable=SC2012

# vm.sh: An opinionated helper script to manage libvirt vms.
# this script assumes that virt-install can create nat-based networking,
# such as VIRSH_DEFAULT_CONNECT_URI=qemu:///system (or root),
# and won't work with qemu://session user-mode networking.

set -euo pipefail

SCRIPTNAME="vm.sh"
SCRIPTFILE="$(realpath "$(dirname "$0")")/$SCRIPTNAME"
if ! [ -v VIRSH_DEFAULT_CONNECT_URI ]; then export VIRSH_DEFAULT_CONNECT_URI="qemu:///system"; fi
VMNAMEPATTERN="vm*"
VMUSER="admin"
VMDIR="${VMDIR:-/home/_vms}"  # VMDIR="/var/lib/libvirt/images"

#echo "uri=$VIRSH_DEFAULT_CONNECT_URI, p=$VMNAMEPATTERN, u=$VMUSER, dir=$VMDIR"

fatal_error() {
  echo "$@">&2; exit 1
}

# update ssh "hosts" definition (for ssh/rsync)
# ~/.ssh/config.d/vms is meant to be included from .ssh/config
#   it is atomically updated by this function using:
# . a list of DHCP leases, from virsh domifaddr
# . a list of static IPs, from ~/.ssh/config.d/static.txt
# files used:
# static.txt is a "db" of static IPs, updated on add/remove
# vms.lock is used for atomic updates
# vms.new and static.txt.new are used as temp files during updates (within lock)
# Note that, while powered off, VMs with DHCP have no IP.
#
# many other approaches are possible here, including something with dnsmasq
# (/var/lib/libvirt/dnsmasq/default.conf,default.addnhosts,default.hostsfile)
# but if ssh-onl resolution is acceptable, this is good enough.
update_ssh_hosts() {
  local add_static=
  local del_static=
  while [ $# -gt 0 ]; do
    local optarg=
    optarg="$(echo "$1" | sed 's/[-_a-zA-Z0-9]*=//')"
    case "$1" in
      --add-static=*) add_static="$optarg" ;;
      --del-static=*) del_static="$optarg" ;;
      -*) fatal_error "unknown option '$1'" ;;
      *) break ;;
    esac
    shift 1
  done
  mkdir -p ~/.ssh/config.d
  (
    # acquire exclusive lock for .ssh/config.d vms files modification
    flock -w 5 -x 200
    local res=$?
    if [ $res -ne 0 ]; then
      fatal_error "update_ssh_hosts: failed getting lock"
    fi
    # lock aquired, run commands
    local staticfile="$HOME/.ssh/config.d/static.txt"
    if [ -n "$add_static" ]; then
      touch "$staticfile"
      local host=
      host="$(echo "$add_static" | cut -d, -f1)"
      local ip=
      ip="$(echo "$add_static" | cut -d, -f2)"
      grep -v "^$host " "$staticfile" > "$staticfile.new" || true
      mv "$staticfile.new" "$staticfile"
      echo "$host $ip" >>"$staticfile"
    elif [ -n "$del_static" ]; then
      touch "$staticfile"
      local host="$del_static"
      grep -v "^$host " "$staticfile" > "$staticfile.new" || true
      mv "$staticfile.new" "$staticfile"
    fi
    (cat <<EOF
# This file was auto-generated by "vm.sh update_ssh_hosts"

Host $VMNAMEPATTERN
StrictHostKeyChecking No
UserKnownHostsFile /dev/null

EOF
     cmd_list_all_ips "$@") >~/.ssh/config.d/vms.new && mv ~/.ssh/config.d/vms.new  ~/.ssh/config.d/vms
  ) 200>~/.ssh/config.d/vms.lock
}

# return 0 if arg is a integer + "G", return 1 otherwise
disk_def_is_size() {
  local def="$1"
  case "$def" in
    *G) def="${def%G}" ;;
    *) return 1 ;;
  esac
  if test "$def" -eq "$def" 2>/dev/null; then
    return 0
  fi
  return 1
}

cmd_create() { ##HELP create [options] <vmname>
  local noshell=false
  local allow_root=false
  local nest=false
  local ostype="centos"
  # domain used to be "lan", but inconsistent with vm name in DNS lookups
  local domain=""
  local passwd=""
  local vcpus=2
  local memory=4096
  local disk_def="80G"
  local staticip=
  local network="default"
  local cleaniso=false
  local cimode="iso"
  # see also vip_virt.sh for options
  while [ $# -gt 0 ]; do
    local optarg=
    optarg="$(echo "$1" | sed 's/[-_a-zA-Z0-9]*=//')"
    case "$1" in
      --noshell) noshell=true ;;
      --cleaniso) cleaniso=true ;;
      --cloud-init=*) cimode="$optarg" ;;
      --allow-root) allow_root=true ;;
      --nest) nest=true ;;
      --user=*) VMUSER="$optarg" ;;
      --os=*) ostype="$optarg" ;;
      --domain=*) domain="$optarg" ;;
      --passwd=*) passwd="$optarg" ;;
      --vcpus=*) vcpus="$optarg" ;;
      --memory=*) memory="$optarg" ;;
      --disk=*) disk_def="$optarg" ;;
      --network=*) network="$optarg" ;;
      --staticip=*) staticip="$optarg"
        case "$staticip" in
          192.168.122.*) ;;
          *) fatal_error "unsupported IP '$staticip'" ;;
        esac
        ;;
      -*) fatal_error "unknown option '$1'" ;;
      *) break ;;
    esac
    shift 1
  done
  case "$cimode" in
    direct|http|iso) ;;
    *) fatal_error "unknown cloud-init mode '$cimode'" ;;
  esac
  if [ $# -ne 1 ]; then
    fatal_error "usage: create [options] <vmname>"
  fi
  local vmname="$1"
  local osvariant=  # from: virt-install --os-variant list
  local url=        # from web search "<distrib_name> cloud image"
  local has_graphics=false
  local ethname="eth0"
  local vmgroups='"wheel"'
  local opts=()
  case "$ostype" in
    # alpine uses https://gitlab.alpinelinux.org/alpine/cloud/tiny-cloud, which doesn't seem to support hostname,staticip,rootpass... (username+ssh is ok)
    # doas apk update; doas apk add docs
    alpine) osvariant="alpinelinux3.19"; url="https://dl-cdn.alpinelinux.org/alpine/v3.21/releases/cloud/generic_alpine-3.21.2-x86_64-bios-tiny-r0.qcow2" ;;
    arch) osvariant="archlinux"
      # https://gitlab.archlinux.org/archlinux/arch-boxes/-/packages
      # Arch-Linux-x86_64-cloudimg-20250601.358142.qcow2
      # sudo pacman -S man
      url="https://gitlab.archlinux.org/archlinux/arch-boxes/-/package_files/9506/download"
      ;;
    centos7) osvariant=centos7; url="https://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-2111.qcow2" ;;
    centos|centos9) osvariant=centos-stream9; url="https://cloud.centos.org/centos/9-stream/x86_64/images/CentOS-Stream-GenericCloud-x86_64-9-20250520.0.x86_64.qcow2" ;;
    centos10) osvariant=centos-stream10 url="https://cloud.centos.org/centos/10-stream/x86_64/images/CentOS-Stream-GenericCloud-10-20250506.2.x86_64.qcow2" ;;
    fedora) osvariant="fedora41"; url="https://download.fedoraproject.org/pub/fedora/linux/releases/41/Cloud/x86_64/images/Fedora-Cloud-Base-Generic-41-1.4.x86_64.qcow2"; ethname="enp1s0" ;;
    almalinux) osvariant=almalinux8; url="https://repo.almalinux.org/almalinux/8/cloud/x86_64/images/AlmaLinux-8-GenericCloud-8.10-20240819.x86_64.qcow2" ;;
    ubuntu2604)
      # 26.04: https://cloud-images.ubuntu.com/resolute/current/, first images just available early 202411, dev branch, no osvariant yet in libvirt
      osvariant="ubuntu25.10"; url="https://cloud-images.ubuntu.com/resolute/current/resolute-server-cloudimg-amd64.img"; vmgroups='"sudo"'; ethname="enp1s0" ;;
    ubuntu|ubuntu2404) osvariant="ubuntu24.04"; url="https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img"; vmgroups='"sudo"'; ethname="enp1s0" ;;
    ubuntu2204) osvariant="ubuntu22.04"; url="https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img"; vmgroups='"sudo"'; ethname="enp1s0" ;;
    ubuntu2004) osvariant="ubuntu20.04"; url="https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img"; vmgroups='"sudo"'; ethname="enp1s0" ;;
    # debian: use "generic" images, not "genericcloud",
    # for cloud-init+libvirt to work
    # sudo apt update; sudo apt install man
    debian|debian12) osvariant="debian12"; url="https://cloud.debian.org/images/cloud/bookworm/20241125-1942/debian-12-generic-amd64-20241125-1942.qcow2"; vmgroups='"sudo"'; ethname="enp1s0" ;;
    debian13) osvariant="debian13"; url="https://cloud.debian.org/images/cloud/trixie/20251006-2257/debian-13-generic-amd64-20251006-2257.qcow2"; vmgroups='"sudo"'; ethname="enp1s0"
      # workaround debian13 stuck on "Booting `Debian GNU/Linux" with the default type=serial console: https://www.mail-archive.com/debian-bugs-dist@lists.debian.org/msg2051511.html
      opts+=("--console=pty,target.type=virtio")
    ;;
    opensuse) osvariant="opensusetumbleweed"; has_graphics=true
      url="https://download.opensuse.org/tumbleweed/appliances/openSUSE-Tumbleweed-Minimal-VM.x86_64-Cloud.qcow2"
      # needs a graphics else doesn't boot
      # also opensuse "leap", more server-oriented with a slower release cycle
      # sudo zypper install man
      ;;
    *) fatal_error "unknown os $2" ;;
  esac
  echo "cd $VMDIR/vms" && cd "$VMDIR/vms" || exit 1
  local use_baseimg=false
  if disk_def_is_size "$disk_def"; then
    use_baseimg=true
  fi
  local osbaseimg=$osvariant.src.qcow2
  if ! [ -e "../oses/$osbaseimg" ] && "$use_baseimg"; then
    echo "downloading $osvariant..."
    wget "$url" -O "../oses/$osbaseimg"
    chmod 444 "../oses/$osbaseimg"  # ownership will auto-change to qemu:qemu
  fi
  # make cloud-init iso
  # virt-install also now has a --cloud-init parameter
  local cidiso="$vmname-cidata.iso"
  mkdir -p "$vmname-cidata"
  (cd "$vmname-cidata"
  cat <<EOF >meta-data
instance-id: $vmname
local-hostname: $vmname
EOF
  cat <<EOF >user-data
#cloud-config
users:
  - name: $VMUSER
    shell: /bin/bash
    ssh_authorized_keys:
      - $(cat "$(ls ~/.ssh/id_*.pub | head -n1)")
EOF
  # forward all host authorized_keys to guest (for nested vms)
  if [ -e ~/.ssh/authorized_keys ]; then
    grep '^ssh' ~/.ssh/authorized_keys | sed 's/^/      - /' >>user-data || true
  fi
  cat <<EOF >>user-data
    groups: [$vmgroups]
    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
EOF
  # ubuntu already has an "admin" group, which makes default "admin" user fail
  if [[ "$ostype" =~ ubuntu.* ]] && [ "$VMUSER" = "admin" ]; then
    echo "    no_user_group: true" >>user-data
  fi
  if [ -n "$domain" ]; then
    cat <<EOF >>user-data
preserve_hostname: False
hostname: $vmname
fqdn: $vmname.$domain
EOF
  fi
  if [ -n "$passwd" ]; then
    cat <<EOF >>user-data
chpasswd:
  expire: False
  users:
  - {name: $VMUSER, password: $passwd, type: text}
EOF
  fi

  # password auth
# local vmpass=$(echo a | mkpasswd -m sha-512 -s)
# one or several of:
# (lock default may be os-dependend, also this is only local and not ssh)
#  passwd: "$vmpass"
#  lock_passwd: false
#  chpasswd: { expire: False }
# set root password for debugging:
#chpasswd:
#  expire: False
#  list: |
#     root:root
#  users:
#  - {name: admin, password: a, type: text}

  # IPv4 config: dhcp or static
  # this hardcodes common default values for the libvirt network "default":
  # . virsh network bridge interface: virbr0
  #   virsh net-info default | grep Bridge | awk '{print $2}'
  # . bridge interface address (gateway): 192.168.122.1/24:
  #   ip -j addr show dev virbr0 | jq -r '.[0].addr_info[0].local'
  #   ip -j addr show dev virbr0 | jq -r '.[0].addr_info[0].prefixlen'
  # these can be checked with "virsh net-dumpxml default"
  if [ -n "$staticip" ]; then # static IPv4
    cat <<EOF >network-config
network:
  version: 2
  ethernets:
    $ethname:
      dhcp4: no
      addresses: [$staticip/24]
      nameservers:
           addresses: [192.168.122.1]
      routes:
      - to: 0.0.0.0/0
        via: 192.168.122.1
EOF
  else
    touch network-config
  fi
  touch vendor-data
  )
  # list of cloud-init files
  local cifiles=("user-data" "meta-data")
  cifiles+=("network-config")
  cifiles+=("vendor-data")

  # adjust launch options
  # some gotchas:
  # . waiting for IP assumes host-nat-based networking
  # . virt-install uses qemu:///session by default even with VIRSH_DEFAULT_CONNECT_URI so we need an explicit --connect= :
  if ! $has_graphics; then opts+=("--graphics=none"); fi
  opts+=("--network=$network")
  # cloud-init mode
  local ciip=
  local ciport=
  local cipid=
  if [ "$cimode" = "direct" ]; then # direct files
    # libvirt currently implements this with an automatic iso, which is unmounted and deleted post-install. But the cdrom device remains, and this init mode prevents first reboot on the machine. So we prefer the explicit iso below.
    opts+=("--cloud-init=$(for f in "${cifiles[@]}"; do echo "$f=$vmname-cidata/$f"; done | head -c -1 | tr '\n' ',')")
  elif [ "$cimode" = "http" ]; then
    ciip=$(ip -j addr show dev virbr0 | jq -r '.[0].addr_info[0].local')
    # get an available TCP port for cloud-init http.server.
    # This is a bit racy as we don't flock.
    # "python3 -m http.server 0" automatically binds to an available port,
    # but then it's not ovious how to get it: see also ss/netstat/lsof.
    ciport=$(for ((p=8000;p<8100;p++)); do if ! ss -lnt4 | grep -q ":$p "; then echo $p; break; fi; done)
    if [ -z "$ciport" ]; then fatal_error "can't find a port for cloud-init http.server"; fi
    python3 -m http.server --bind="$ciip" --directory="$vmname-cidata" "$ciport" &
    cipid=$!
    opts+=("--qemu-commandline=-smbios type=1,serial=ds=nocloud;s=http://$ciip:$ciport/")
  else # "$cimode" = iso, create cloud-init iso
    (cd "$vmname-cidata"
    mkisofs -quiet -o "$cidiso" -V cidata -J -r "${cifiles[@]}"
    mv "$cidiso" ../
    # alt: genisoimage -output cidata.iso -input-charset utf-8 -V cidata -r -J user-data meta-data
    )
    rm -r "$vmname-cidata"
    opts+=("--disk=path=$vmname-cidata.iso,device=cdrom")
  fi
  # alt cloud-init: "nocloud" http source, see https://cloudinit.readthedocs.io/en/latest/tutorial/qemu.html

  local diskarg=
  if "$use_baseimg"; then
    # create qcow2 disk image with specified size, backed by readonly OS image
    qemu-img create -b "../oses/$osbaseimg" -f qcow2 -F qcow2 "$vmname.qcow2" "$disk_def"
    diskarg="$PWD/$vmname.qcow2"
  else
    diskarg="$disk_def"
  fi
  virt-install --connect="$VIRSH_DEFAULT_CONNECT_URI" --import --name="$vmname" --memory="$memory" --vcpus="$vcpus" --os-variant="$osvariant" --disk="$diskarg" "${opts[@]}" --noautoconsole

  # wait for network+ssh to be ready
  if [ -n "$staticip" ]; then # staticip
    update_ssh_hosts --add-static="$vmname,$staticip"
  fi
  cmd_ssh --verbose "$vmname" true
  # update ssh config, unless libvirt nss is active
  if ! grep '^hosts:' /etc/nsswitch.conf  | grep -q libvirt; then
    update_ssh_hosts
  fi
  # cleanup cloud-init temp files
  rm -rf "$vmname-cidata"
  if [ -n "$cipid" ]; then
    echo "shutting down cloud-init server on $ciip:$ciport"
    kill "$cipid"
    #kill %% && wait %%
  fi
  # optional cleanup if cimode=iso: this is only effective on next boot,
  # and needs full poweroff, not just reboot:
  if $cleaniso; then
    # or: virsh detach-device "$vmname" <(virsh dumpxml "$vmname" | sed -n '/disk.*cdrom/,/\/disk/p') --config
    virsh detach-disk --domain "$vmname" --config sda
    rm -fv "$vmname-cidata.iso"
    virsh shutdown "$vmname"
    until test "$(virsh domstate "$vmname" | head -1)" = "shut off"; do sleep 0.5; done
    virsh start "$vmname"
    cmd_ssh "$vmname" true
  fi
  # allow root access
  if "$allow_root"; then cmd_allow_root "$VMUSER@$vmname"; fi
  # nest vm.sh
  if "$nest"; then rsync "$SCRIPTFILE" "$VMUSER@$vmname:$SCRIPTNAME"; fi
  # open a shell if requested
  if ! $noshell; then cmd_ssh "$vmname"; fi
}

# get vm IPv4
cmd_get_ip() { ##HELP get_ip <vmname>
  local timeout=0 # infinite
  while [ $# -gt 0 ]; do
    local optarg=
    optarg="$(echo "$1" | sed 's/[-_a-zA-Z0-9]*=//')"
    case "$1" in
      --timeout=*) timeout="$optarg" ;;
      -*) fatal_error "get_ip: unknown option '$1'" ;;
      *) break ;;
    esac
    shift 1
  done
  if [ $# -ne 1 ]; then
    fatal_error "usage: get_ip <vmname>"
  fi
  local vmname="$1"
  shift 1

  local staticfile="$HOME/.ssh/config.d/static.txt"
  if [ -e "$staticfile" ] && grep -q "^$vmname " "$staticfile"; then
    grep "^$vmname " "$staticfile" | sed "s/^$vmname //"
    return 0
  fi
  # wait for an IPv4 to exist
  if [ "$timeout" -gt 0 ]; then
    timeout "$timeout" sh -c "until virsh domifaddr \"$vmname\" | grep ipv4 >/dev/null; do sleep 1; done"
  else
    until virsh domifaddr "$vmname" | grep ipv4 >/dev/null; do sleep 1; done
  fi

  # get the IP on stdout
  virsh domifaddr "$vmname" | grep ipv4 | awk -F' ' '{print $4}' | sed 's/\/.*//'
}

# list IPv4 for all VMs
cmd_list_all_ips() { ##HELP list_all_ips
  # can't use --all here if there are shutdown vms
  virsh --connect="$VIRSH_DEFAULT_CONNECT_URI" list --name | while read -r name; do
    # shellcheck disable=SC2254
    case "$name" in
      $VMNAMEPATTERN)
        # this is called within lock, and machines are supposed to be up,
        # so don't wait for too long, and just skip hosts without an IP
        local vmip=
        vmip="$(cmd_get_ip --timeout=3 "$name" || true)"
        if [ "$vmip" = "" ]; then continue; fi
        echo "Host $name"
        echo "Hostname $vmip"
        echo ""
        ;;
    esac
  done
}

# list all vms (on or off), and their description
cmd_list() { ##HELP list
  virsh --connect="$VIRSH_DEFAULT_CONNECT_URI" list --name --all | while read -r name; do
    # shellcheck disable=SC2254
    case "$name" in
      $VMNAMEPATTERN)
        echo "$name $(virsh dumpxml --domain "$name" | grep "<description>" | sed 's/  <description>//' | sed 's/<\/description>//')"
        ;;
    esac
  done
}

# ssh to vm
cmd_ssh() { ##HELP ssh [options] <vmname> [cmd args..]
  local verbose=false
  local user="$VMUSER"
  while [ $# -gt 0 ]; do
    case "$1" in
      --verbose) verbose=true ;;
      -*) fatal_error "unknown option '$1'" ;;
      *) break ;;
    esac
    shift 1
  done
  if [ $# -lt 1 ]; then
    fatal_error "usage: ssh <vmname> [cmd args...]"
  fi
  local vmname="$1"
  shift 1

  # wait for an IPv4 to exist
  if $verbose; then echo "$vmname: waiting for vm IP..."; fi
  local vmip=
  case "$network" in
    bridge=*) # XXX hacky condition, static & co
      until vmip=$(getent ahostsv4 "$vmname" | grep STREAM | cut -d' ' -f1); do sleep 1; done
      ;;
    *)
      until vmip=$(cmd_get_ip "$vmname"); do sleep 1; done
      ;;
  esac

  # wait for SSH to be up
  if $verbose; then echo "$vmname: found vm IP: $vmip, now waiting for SSH..."; fi
  until ssh -q -o ConnectTimeout=3 -o ConnectionAttempts=1 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null "$user@$vmip" true; do sleep 1; done

  # do connect
  if $verbose; then echo "ssh ready: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null $user@$vmip"; fi
  ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR "$user@$vmip" "$@"
}

# destroy vm and its data
cmd_delete() { ##HELP delete <vmname>
  if [ $# -ne 1 ]; then
    fatal_error "usage: delete <vmname>"
  fi
  local vmname="$1"
  if [ "$(virsh domstate "$vmname")" == "running" ]; then
    virsh destroy "$vmname"
  fi
  # depending on snapshot statuses, this doesn't necessarily delete all snapshot storage. But there should be a libvirt message indicating those.
  # Alternatively we could list and delete all snapshots,
  # or rm -f $VMDIR/vms/${vmname}*
  virsh undefine "$vmname" --remove-all-storage
  # nope: --snapshots-metadata --delete-storage-volume-snapshots
  update_ssh_hosts --del-static="$vmname"
}

# manage vm snapshots
cmd_snapshot() { ##HELP snapshot <vmname> <command>
  if [ $# -eq 1 ] && [ "$1" == "help" ]; then
    echo "usage: snapshot <vmname> <command>..."
    echo "  snapshot <vmname> create <snapshot-name>"
    echo "  snapshot <vmname> revert <snapshot-name>"
    echo "  snapshot <vmname> delete <snapshot-name>"
    echo "  snapshot <vmname> list"
    return 0
  fi
  if [ $# -lt 2 ]; then
    fatal_error "usage: snapshot <vmname> <command>..."
  fi
  local vmname="$1"
  local cmd="$2"
  shift 2
  case "$cmd" in # commands with an extra snapshot name arg
  create|revert|delete)
    if [ $# -ne 1 ]; then
      fatal_error "usage: $cmd <snapshot-name>"
    fi
    local snapname="$1"
    shift 1
    case "$cmd" in
    create)
      local snapfile="$VMDIR/vms/$vmname.$snapname.snapshot"
      virsh snapshot-create-as "$vmname" --name="$snapname" --live --memspec file="$snapfile"
      ;;
    revert) virsh snapshot-revert "$vmname" "$snapname" ;;
    delete) virsh snapshot-delete "$vmname" "$snapname" ;;
    esac
    ;;
  list)
    virsh snapshot-list "$vmname" --name
    ;;
  *) fatal_error "unknown snapshot command '$cmd'" ;;
  esac
}

# allow root access over ssh
cmd_allow_root() {
  if [ $# -ne 1 ]; then
    fatal_error "usage: allow_ssh_root [user@]<vm>"
  fi
  local user="admin"
  local vmname="$1"
  case "$vmname" in
    *@*)
      user=$(echo "$vmname" | cut -d@ -f1)
      vmname=$(echo "$vmname" | cut -d@ -f2)
      ;;
  esac
  if ! ssh "$user@$vmname" "sudo test -e /root/.ssh/config"; then
    echo "$vmname: allow ssh to root"
    ssh "$user@$vmname" "sudo rm -rf /root/.ssh && touch .ssh/config && sudo cp -a .ssh /root/.ssh && sudo chown -R root:root /root/.ssh"
  fi
}

pkg_install() {
  local pkg="$1"
  if ! dnf list installed "$pkg" >/dev/null 2>&1; then
    sudo dnf -y install "$pkg"
  fi
}

set_env() {
  if [ $# -ne 2 ]; then fatal_error "usage: set_env name value"; fi
  local name="$1"
  local value="$2"
  if ! grep -q "export $name=\"$value\"" ~/.bashrc; then
    echo "Adding $name=$value to ~/.bashrc"
    echo "export $name=\"$value\"" >> ~/.bashrc
  fi
  if [ -z "$name" ] || [ "$name" != "$value" ] ; then
    echo "Exporting $name=$value"
    export "$name"="$value"
  fi
}

# check host system and configure as needed
cmd_setup() {
  # ssh
  if ! [ -e "$HOME/.ssh/id_rsa" ]; then
    echo "Generating ssh key"
    ssh-keygen -t rsa -b 4096 -f "$HOME/.ssh/id_rsa" -q -N ""
  fi
  if ! [ -e "$HOME/.ssh/config" ] || ! grep -q "Include $HOME/.ssh/config.d/vms" "$HOME/.ssh/config"; then
    echo "Adjusting ~/.ssh/config for vms"
    echo "Include $HOME/.ssh/config.d/vms" >>"$HOME/.ssh/config"
  fi
  # system packages
  local packages="qemu-img qemu-kvm wget tree libvirt-daemon libvirt-daemon-driver-qemu libvirt-daemon-driver-storage-disk libvirt-daemon-config-network virt-install"
  # optional: virt-manager, guestfs-tools (virt-customize), libvirt-nss
  # shellcheck disable=SC2086
  if [ "$( (dnf list installed $packages 2>/dev/null || true) | tail -n+2 | wc -l)" != 9 ]; then
    echo "Install system packages"
    for pkg in $packages; do pkg_install "$pkg"; done
  fi
  if ! groups | xargs -n1 echo | grep -q '^libvirt$'; then
    if grep '^libvirt' /etc/group | grep -q "$(whoami)"; then
      echo "Logout/login to reload libvirt group"
    else
      echo "Adding current user to group libvirt"
      sudo usermod -aG "libvirt" "$(whoami)"
    fi
  fi # or install libvirt-nss
  # libvirtd service
  if ! systemctl is-active libvirtd >/dev/null; then
    echo "Starting libvirtd.service"
    sudo systemctl start libvirtd
  fi
  # create ~/.ssh/config.d/vms, for StrictHostKeyChecking=No and UserKnownHostsFile=/dev/null on name pattern
  if ! [ -e "$HOME/.ssh/config.d/vms" ]; then
    echo "Create ~/.ssh/config.d/vms"
    update_ssh_hosts
  fi
  # env&dir
  set_env "VMDIR" "/home/vms"
  set_env "VIRSH_DEFAULT_CONNECT_URI" "qemu:///system"
  if ! [ -e "$VMDIR" ]; then
    echo "Creating directory '$VMDIR'"
    sudo mkdir -p "$VMDIR" "$VMDIR/"{vms,oses,disks,clusters} -m 775
    sudo chown -R "$(whoami):libvirt" "$VMDIR/"
  fi
  # need login/logout for groups update and bashrc reload
}

# show help
cmd_help() { ##HELP help
  echo "usage: vm.sh <command>"
  echo " commands:"
  grep '#''#HELP' "$0" | sed 's/.*#''#HELP//' | sort
}

# main
if [ $# -lt 1 ]; then
  fatal_error "usage: vm.sh <command>"
fi
cmd=$1
shift
case "$cmd" in
  create) cmd_create "$@" ;;
  delete) cmd_delete "$@" ;;
  files) tree -a "$VMDIR" ;;
  get_ip) cmd_get_ip "$@" ;;
  allow_root) cmd_allow_root "$@" ;;
  help) cmd_help "$@" ;;
  list) cmd_list "$@" ;;
  list_all_ips) cmd_list_all_ips "$@" ;;
  update_hosts) update_ssh_hosts ;;
  setup) cmd_setup "$@" ;;
  shell|ssh) cmd_ssh "$@" ;;
  snapshot) cmd_snapshot "$@" ;;
  *) fatal_error "unknown command '$cmd', try 'help'" ;;
esac
